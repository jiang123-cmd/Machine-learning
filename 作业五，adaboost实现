import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import math

from IPython.core import debugger
debug = debugger.Pdb().set_trace

%matplotlib inline


# 问题描述：实现SVM的SMO优化算法，训练给定数据集，并测试和可视化结果。对比与sklearn科学计算工具包的svm方法效果。


# 鸢尾花(iris)数据集
# 数据集内包含 3 类共 150 条记录，每类各 50 个数据，
# 每条记录都有 4 项特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度，
# 可以通过这4个特征预测鸢尾花卉属于（iris-setosa, iris-versicolour, iris-virginica）中的哪一品种。
# 这里只取前100条记录，两项特征，两个类别。

def create_data():
    iris = load_iris()
    df = pd.DataFrame(iris.data, columns=iris.feature_names)
    df['label'] = iris.target
    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']
    data = np.array(df.iloc[:100, [0, 1, -1]])
    for i in range(len(data)):
        if data[i,-1] == 0:
            data[i,-1] = -1
    # print(data)
    return data[:,:2], data[:,-1]

X, y = create_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=65)

x=[0,1,2,3,4,5,6,7,8,9]
x=np.array(x)
y=[1,1,1,-1,-1,-1,1,1,1,-1]
y=np.array(y)
class Adaboost():
    def __init__(self):
        self=self
    #求阈值,误差率,Gm(x)
    def threshold(self, weight,y_train):
        self.long=len(weight)
        self.distinguish=[1,-1]
        self.result=[]
        self.v_best=0
        self.error_rate=0
        self.error_rate_min=1
        self.v=0
        self.c=0
        for i in range(self.n):
            self.result+=[0]
        for i in range(self.long-1):
            self.error_rate=0
            self.v=i+0.5
            #分类后的标签值
            for j in range(i):
                self.result[j]=y_train[i]
            for j in range(i+1,self.long):
                self.result[j]=y_train[i+1]
            #误差率
            for m in range(self.long):
                if self.result[m]!=y_train[m]:
                    self.error_rate=self.error_rate+weight[m]
            #最小误差率
            if (self.error_rate<self.error_rate_min):
                self.c+=1
                self.error_rate_min=self.error_rate
                self.v_best=self.v
                #self.result_best=self.result #基本分类器
                self.distinguish[0]=y_train[i]
                self.distinguish[1]=-self.distinguish[0]
        #print("v: ",self.v_best)
        for j in range(int(self.v_best-0.5)):
            self.result[j]=y_train[(int(self.v_best-0.5))]
        #print("result",y_train[(int(self.v_best-0.5))])
        for j in range(int(self.v_best+0.5),self.long):
            self.result[j]=y_train[int(self.v_best+0.5)]
        return self.v_best,self.error_rate_min,self.result,self.distinguish
    
    
    #求G(x)的系数apha,对的
    def apham(self,error_rate):
        self.apha=(math.log((1-error_rate)/error_rate))/2
        return self.apha
    
    
    #求新的权值分布
    def omiga(self,weight_1,apha,y_train,Gm_1):
        self.weight_2=[]
        self.Z=0
        self.long=len(weight_1)
        for i in range(self.long):
            self.Z=self.Z+weight_1[i]*math.exp(-apha*y_train[i]*Gm_1[i])
        for i in range(self.long):
            self.weight_2+=[weight_1[i]*math.exp(-apha*y_train[i]*Gm_1[i])/self.Z]
        return self.weight_2
    
    
    def train(self,x,y):
        self.n=len(y)
        self.Threshol=[]#阈值
        self.weight=[]#权重
        self.apha=[]
        self.Gm=[]
        self.Z=[]
        self.distinguish=[]#两种情况：[1,-1],[-1,1]
        self.distinguish_sum=[]
        self.error_quantity=1
        self.m=0 #基本分类器的标号
        self.result_final=[]#分类结果
        self.result_finally=[]
        self.frequency=0
        for i in range(self.n):
            self.weight+=[round(1/self.n,4)]
            self.result_final+=[0]
            self.result_finally+=[0]
        #更新的只有权重，通过权重和y的函数产生Gm,权重是为了求apha的，与最终结果无关
        while (self.error_quantity!=0):
            
            self.error_quantity=0
            self.Threshol_1,self.error_rate,self.Gm,self.distinguish=self.threshold(self.weight,y)
            
            self.Threshol+=[self.Threshol_1]
            self.distinguish_sum+=[self.distinguish]
            self.apha+=[self.apham(self.error_rate)]
            #print("w_1: ",np.array(self.weight))
            #print("apha: ",self.apha[self.m])
            #print("Gm",np.array(self.Gm))
            #print("y: ",y)
            self.weight=self.omiga(np.array(self.weight),self.apha[self.m],y,np.array(self.Gm))
            #print("w: ",self.weight)
            #i是基本分类器的个数，从1开始，每个基本分类器apha*Gm
            #print("m:",self.m)
            #print("apha: ",self.apha[self.m])
            #print("distinguish: ",self.distinguish[0])
            
            for j in range(int(self.Threshol[self.m]+0.5)):
                self.result_final[j]=self.apha[self.m]*self.distinguish[0]+self.result_final[j]
                    #print("result_float: ",self.result_final[j])
            for j in range(int(self.Threshol[self.m]+0.5),self.n):
                self.result_final[j]=self.apha[self.m]*self.distinguish[1]+self.result_final[j]
            #print("Threshol: ",int(self.Threshol[self.m]+0.5))
            #print("n",self.n)
            
            
            for i in range(self.n):
                if self.result_final[i]<0:
                    self.result_finally[i]=-1
                else:
                    self.result_finally[i]=1
                if (self.result_finally[i]!=y[i]):
                    self.error_quantity+=1
            #print("result",self.result_final)
            print("error_quantity: ",self.error_quantity)
            
            #print("错误分类的数量：",self.error_quantity)
            self.m+=1
            self.frequency+=1
            #print("基本分类器数量：",self.frequency)
            if (self.frequency>=100):#最多100个基本分类器
                break
        return self.apha,self.Threshol,self.distinguish_sum,self.result_finally
            
adaboost=Adaboost()
apha=[]
Threshol=[]
distinguish_sum=[]
result=[]
apha,Threshol,distinguish_sum,result=adaboost.train(X_train,y_train)
print("apha: ",apha)  #每个弱分类器的apha
print("Threshol: ",Threshol)  #每个弱分类器的阈值
print("distinguish_sum: ",distinguish_sum)  #每个弱分类器阈值左右的数
#最好的结果有9个误分类的点

print(np.array(result))
for i in range(len(result)):
    y_train[i]=result[i]
    plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], marker='o', c='r', label='1') 
    plt.scatter(X_train[y_train==-1,0], X_train[y_train==-1,1], marker='o', c='b', label='-1') 
    
 2
比较支持向量机、 AdaBoost 、逻辑斯谛回归模型的学习策略与算法。
支持向量机:
学习策略:就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。
算法：SMO算法：SMO算法是是一种启发式算法，其基本思路是：如果所有变量的解都满足此最优化问题的KKT条件，那么这个最优化问题的解就得到了。
选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问
题的目的。

Adaboost:
学习策略：从弱学习算法出发，反复学习，得到一系列弱分类器（又叫基本分类器），然后组合这些弱分类器，构成一个强分类器。
算法：对于如何改变训练数据的权值分布，adaboost提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值; 对于如何将弱
分类器组合成一个强分类器，adaboost使用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起更大的作用。减小分类误差率大
的弱分类器的权值，使其在表决中起较小的作用。

逻辑斯谛回归模型：
学习策略：逻辑斯谛回归模型的学习策略是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计，可以形式化为无约束最优化问题
算法：改进的迭代尺度算法（IIS），梯度下降法，牛顿法以及拟牛顿法
    
